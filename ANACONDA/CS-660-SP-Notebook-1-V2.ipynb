{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-660 Semester Project Notebook 1\n",
    "\n",
    "## Stan Rosenbaum\n",
    "\n",
    "Fall 2017\n",
    "\n",
    "Anaconda 5 / Python 3\n",
    "\n",
    "Using Keras with TensorFlow as back end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we init stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS660DataManagementCheck Imported\n"
     ]
    }
   ],
   "source": [
    "# Load the Basic Python Libraries\n",
    "import os\n",
    "import csv\n",
    "import PIL\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# Load my Data Management Module\n",
    "import CS660DataManagement as csDM\n",
    "\n",
    "# load numpy\n",
    "import numpy as np\n",
    "\n",
    "# Load Keras Studd\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "# Other\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "\n",
    "\n",
    "# Not sure what this is:\n",
    "# from kt_utils import *\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Get Processed Data Directory.\n",
    "processedDataDir = os.path.join( os.getcwd(), os.pardir, 'DATA', 'PROCESSED' )\n",
    "combinedDataDir = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED' )\n",
    "pickleDataDir = os.path.join( os.getcwd(), os.pardir, 'DATA', 'PICKLES' )\n",
    "\n",
    "testImageColorFile = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED', 'ICOLOR', 'TEST.png' )\n",
    "testImageDepthFile = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED', 'IDEPTH', 'TEST.png' )\n",
    "testCSVFile = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED', 'PCLOUD', 'TEST.csv' )\n",
    "\n",
    "combinedDataDir = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED' )\n",
    "\n",
    "imageDirs = ['ICOLOR', 'IDEPTH']\n",
    "csvDirs = ['PCLOUD']\n",
    "\n",
    "allDataFlavors = imageDirs + csvDirs\n",
    "\n",
    "#Load main data file.\n",
    "searCSVInfoFile = os.path.join( combinedDataDir, 'SEAR_DC_INFO.csv' )\n",
    "\n",
    "csDM.CS660DataManagementCheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Learning and Verification sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n",
      "Number of Everything\n",
      "UPs: 278\n",
      "DOWNs: 201\n",
      "NAs: 204\n",
      "HOLEs: 218\n"
     ]
    }
   ],
   "source": [
    "print( len( csDM.getListOfDataCSVFileKeys() ) )\n",
    "csDM.reportStats()\n",
    "#csDM.createLearningAndVerificationPickles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Training Set.\n",
    "#### (Basing this all on the Coursera Course Code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 100\n",
      "number of test examples = 20\n",
      "X_train shape: (100, 480, 640, 1)\n",
      "Y_train shape: (100, 1)\n",
      "y_train_binary shape: (100, 4)\n",
      "X_test shape: (20, 480, 640, 1)\n",
      "Y_test shape: (20, 1)\n",
      "y_test_binary shape: (20, 4)\n"
     ]
    }
   ],
   "source": [
    "def reshape(trainingSet):\n",
    "    \"\"\"\n",
    "    This is to add the 'Channels' Dimension for Keras.\n",
    "    \"\"\"\n",
    "    getShape = trainingSet.shape\n",
    "    newShape = getShape + (1,)\n",
    "    return trainingSet.reshape(newShape)\n",
    "    \n",
    "\n",
    "X_train, Y_train, X_test, Y_test = csDM.load_dataset( '1', 'ICOLOR' )\n",
    "\n",
    "\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "\n",
    "X_train = reshape(X_train)\n",
    "X_test = reshape(X_test)\n",
    "\n",
    "y_train_binary = to_categorical(Y_train, 4)\n",
    "y_test_binary = to_categorical(Y_test, 4)\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"y_train_binary shape: \" + str(y_train_binary.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "print (\"y_test_binary shape: \" + str(y_test_binary.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Building a model in Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS660DataManagementCheck Imported\n"
     ]
    }
   ],
   "source": [
    "csDM.CS660DataManagementCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (100, 480, 640, 1)\n",
      "100 train samples\n",
      "20 test samples\n",
      "x_train shape: (100, 480, 640, 1)\n",
      "(100, 4) y_train shape\n",
      "(20, 4) y_test shape\n",
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/32\n",
      "100/100 [==============================] - 16s - loss: 1.8628 - categorical_accuracy: 0.3000 - val_loss: 1.4357 - val_categorical_accuracy: 0.2500\n",
      "Epoch 2/32\n",
      "100/100 [==============================] - 16s - loss: 1.7874 - categorical_accuracy: 0.2300 - val_loss: 1.4272 - val_categorical_accuracy: 0.2500\n",
      "Epoch 3/32\n",
      "100/100 [==============================] - 15s - loss: 1.5364 - categorical_accuracy: 0.2900 - val_loss: 1.3583 - val_categorical_accuracy: 0.2500\n",
      "Epoch 4/32\n",
      "100/100 [==============================] - 15s - loss: 1.3883 - categorical_accuracy: 0.2600 - val_loss: 1.3391 - val_categorical_accuracy: 0.2500\n",
      "Epoch 5/32\n",
      "100/100 [==============================] - 15s - loss: 1.3426 - categorical_accuracy: 0.2500 - val_loss: 1.3009 - val_categorical_accuracy: 0.3000\n",
      "Epoch 6/32\n",
      "100/100 [==============================] - 15s - loss: 1.3245 - categorical_accuracy: 0.3100 - val_loss: 1.2363 - val_categorical_accuracy: 0.2500\n",
      "Epoch 7/32\n",
      "100/100 [==============================] - 16s - loss: 1.2770 - categorical_accuracy: 0.2900 - val_loss: 1.1616 - val_categorical_accuracy: 0.3500\n",
      "Epoch 8/32\n",
      "100/100 [==============================] - 18s - loss: 1.2090 - categorical_accuracy: 0.3700 - val_loss: 1.0721 - val_categorical_accuracy: 0.5000\n",
      "Epoch 9/32\n",
      "100/100 [==============================] - 18s - loss: 1.1455 - categorical_accuracy: 0.4200 - val_loss: 1.0342 - val_categorical_accuracy: 0.5500\n",
      "Epoch 10/32\n",
      "100/100 [==============================] - 18s - loss: 1.1107 - categorical_accuracy: 0.4700 - val_loss: 1.1956 - val_categorical_accuracy: 0.4000\n",
      "Epoch 11/32\n",
      "100/100 [==============================] - 19s - loss: 1.0885 - categorical_accuracy: 0.4700 - val_loss: 1.0149 - val_categorical_accuracy: 0.5000\n",
      "Epoch 12/32\n",
      "100/100 [==============================] - 18s - loss: 1.0194 - categorical_accuracy: 0.6200 - val_loss: 1.0141 - val_categorical_accuracy: 0.7000\n",
      "Epoch 13/32\n",
      "100/100 [==============================] - 17s - loss: 0.9753 - categorical_accuracy: 0.6100 - val_loss: 0.9441 - val_categorical_accuracy: 0.6500\n",
      "Epoch 14/32\n",
      "100/100 [==============================] - 17s - loss: 0.8962 - categorical_accuracy: 0.6800 - val_loss: 1.3733 - val_categorical_accuracy: 0.4500\n",
      "Epoch 15/32\n",
      "100/100 [==============================] - 16s - loss: 0.7425 - categorical_accuracy: 0.7300 - val_loss: 1.9365 - val_categorical_accuracy: 0.4500\n",
      "Epoch 16/32\n",
      "100/100 [==============================] - 17s - loss: 0.6977 - categorical_accuracy: 0.7100 - val_loss: 1.0195 - val_categorical_accuracy: 0.6500\n",
      "Epoch 17/32\n",
      "100/100 [==============================] - 16s - loss: 0.7412 - categorical_accuracy: 0.8200 - val_loss: 1.0098 - val_categorical_accuracy: 0.6500\n",
      "Epoch 18/32\n",
      "100/100 [==============================] - 16s - loss: 0.5880 - categorical_accuracy: 0.7900 - val_loss: 1.5088 - val_categorical_accuracy: 0.6500\n",
      "Epoch 19/32\n",
      "100/100 [==============================] - 17s - loss: 0.5784 - categorical_accuracy: 0.7100 - val_loss: 10.9017 - val_categorical_accuracy: 0.2500\n",
      "Epoch 20/32\n",
      "100/100 [==============================] - 15s - loss: 3.1825 - categorical_accuracy: 0.5200 - val_loss: 1.3739 - val_categorical_accuracy: 0.4000\n",
      "Epoch 21/32\n",
      "100/100 [==============================] - 15s - loss: 1.1018 - categorical_accuracy: 0.4600 - val_loss: 0.9883 - val_categorical_accuracy: 0.6000\n",
      "Epoch 22/32\n",
      "100/100 [==============================] - 15s - loss: 0.8177 - categorical_accuracy: 0.7000 - val_loss: 1.0704 - val_categorical_accuracy: 0.6000\n",
      "Epoch 23/32\n",
      "100/100 [==============================] - 15s - loss: 0.6576 - categorical_accuracy: 0.7300 - val_loss: 1.1306 - val_categorical_accuracy: 0.6000\n",
      "Epoch 24/32\n",
      "100/100 [==============================] - 16s - loss: 0.4975 - categorical_accuracy: 0.8100 - val_loss: 1.3176 - val_categorical_accuracy: 0.6000\n",
      "Epoch 25/32\n",
      "100/100 [==============================] - 15s - loss: 0.4742 - categorical_accuracy: 0.8300 - val_loss: 2.1322 - val_categorical_accuracy: 0.6000\n",
      "Epoch 26/32\n",
      "100/100 [==============================] - 16s - loss: 0.4181 - categorical_accuracy: 0.8200 - val_loss: 1.2438 - val_categorical_accuracy: 0.6000\n",
      "Epoch 27/32\n",
      "100/100 [==============================] - 15s - loss: 0.3188 - categorical_accuracy: 0.9200 - val_loss: 0.9382 - val_categorical_accuracy: 0.6500\n",
      "Epoch 28/32\n",
      "100/100 [==============================] - 15s - loss: 0.2185 - categorical_accuracy: 0.9500 - val_loss: 1.1420 - val_categorical_accuracy: 0.6500\n",
      "Epoch 29/32\n",
      "100/100 [==============================] - 15s - loss: 0.2029 - categorical_accuracy: 0.9500 - val_loss: 1.3362 - val_categorical_accuracy: 0.6500\n",
      "Epoch 30/32\n",
      "100/100 [==============================] - 15s - loss: 0.1208 - categorical_accuracy: 0.9700 - val_loss: 1.6797 - val_categorical_accuracy: 0.6000\n",
      "Epoch 31/32\n",
      "100/100 [==============================] - 15s - loss: 0.2005 - categorical_accuracy: 0.9500 - val_loss: 1.4838 - val_categorical_accuracy: 0.6500\n",
      "Epoch 32/32\n",
      "100/100 [==============================] - 15s - loss: 0.0707 - categorical_accuracy: 0.9900 - val_loss: 1.5676 - val_categorical_accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18bb067b8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from quiver_engine import server\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 480, 640\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "x_train, y_train, x_test, y_test = csDM.load_dataset( '1', 'ICOLOR' )\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_as_category = to_categorical(y_train, num_classes)\n",
    "y_test_as_category = to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(y_train_as_category.shape, 'y_train shape')\n",
    "print(y_test_as_category.shape, 'y_test shape')\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Conv2D(5, \n",
    "           kernel_size=(5, 5),\n",
    "           activation='relu',\n",
    "           input_shape=input_shape\n",
    "          )\n",
    ")\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        10, \n",
    "        kernel_size=(2, 2), \n",
    "        activation='relu')\n",
    ")\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['categorical_accuracy']\n",
    ")\n",
    "\n",
    "# server.launch(model)\n",
    "\n",
    "TBoardCallback = keras.callbacks.TensorBoard(\n",
    "    log_dir='../TENSOR_LOGS/run_test', \n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=True\n",
    ")\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train_as_category,\n",
    "          batch_size=16,\n",
    "          epochs=32,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test_as_category),\n",
    "          callbacks = [TBoardCallback]\n",
    "         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.56756234169\n",
      "Test accuracy: 0.699999988079\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test_as_category, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"../MODELS/modelTEST.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"../WEIGHTS/modelTEST.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 480, 640, 1)\n",
      "20/20 [==============================] - 1s\n",
      "[2 2 1 3 1 2 1 0 1 0 2 3 3 3 3 3 1 0 1 0]\n",
      "prediction of:  2  VS  [ 2.]\n",
      "prediction of:  2  VS  [ 2.]\n",
      "prediction of:  1  VS  [ 1.]\n",
      "prediction of:  3  VS  [ 2.]\n",
      "prediction of:  1  VS  [ 1.]\n",
      "prediction of:  2  VS  [ 1.]\n",
      "prediction of:  1  VS  [ 2.]\n",
      "prediction of:  0  VS  [ 0.]\n",
      "prediction of:  1  VS  [ 1.]\n",
      "prediction of:  0  VS  [ 1.]\n",
      "prediction of:  2  VS  [ 2.]\n",
      "prediction of:  3  VS  [ 3.]\n",
      "prediction of:  3  VS  [ 3.]\n",
      "prediction of:  3  VS  [ 3.]\n",
      "prediction of:  3  VS  [ 3.]\n",
      "prediction of:  3  VS  [ 3.]\n",
      "prediction of:  1  VS  [ 0.]\n",
      "prediction of:  0  VS  [ 0.]\n",
      "prediction of:  1  VS  [ 0.]\n",
      "prediction of:  0  VS  [ 0.]\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "print(x_test.shape)\n",
    "results = model.predict_classes(x_test, verbose=1)\n",
    "print(results)\n",
    "for index in range(len(x_test)):\n",
    "    print('prediction of: ', results[index], \" VS \", y_test[index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/staque/Development/GitHub/CS-660-Semester-Project/ANACONDA'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
