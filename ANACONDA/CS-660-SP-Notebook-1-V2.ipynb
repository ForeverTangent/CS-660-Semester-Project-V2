{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-660 Semester Project Notebook 1\n",
    "\n",
    "## Stan Rosenbaum\n",
    "\n",
    "Fall 2017\n",
    "\n",
    "Anaconda 5 / Python 3\n",
    "\n",
    "Using Keras with TensorFlow as back end.\n",
    "\n",
    "### Background Important Stuff\n",
    "\n",
    "First the Enums of the Classes\n",
    "\n",
    "* \"NA\" = 0\n",
    "* \"UP\" = 1\n",
    "* \"DOWN\" = 2\n",
    "* \"HOLE\" = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS660DataManagementCheck Imported\n"
     ]
    }
   ],
   "source": [
    "# First we init stuff.\n",
    "# Load the Basic Python Libraries\n",
    "import os\n",
    "import csv\n",
    "import PIL\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# Load my Data Management Module\n",
    "import CS660DataManagement as csDM\n",
    "\n",
    "# load numpy\n",
    "import numpy as np\n",
    "\n",
    "# Load Keras Stuff\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "# Other.  Mostly Graphic stuff for displaying Data in and out of Jupyter.\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import pydot\n",
    "import graphviz\n",
    "from IPython.display import SVG\n",
    "\n",
    "# Not using Quiver Yet.\n",
    "# from quiver_engine import server\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Get Processed Data Directory.\n",
    "processedDataDir = os.path.join( os.getcwd(), os.pardir, 'DATA', 'PROCESSED' )\n",
    "combinedDataDir = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED' )\n",
    "pickleDataDir = os.path.join( os.getcwd(), os.pardir, 'DATA', 'PICKLES' )\n",
    "modelsDirectory = os.path.join( os.getcwd(), os.pardir, 'MODELS' )\n",
    "modelsStructsDirectory = os.path.join( os.getcwd(), os.pardir, 'MODELS_STRUCTS' )\n",
    "weightsDirectory = os.path.join( os.getcwd(), os.pardir, 'WEIGHTS' )\n",
    "resultsDirectory = os.path.join( os.getcwd(), os.pardir, 'RESULTS' )\n",
    "\n",
    "testImageColorFile = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED', 'ICOLOR', 'TEST.png' )\n",
    "testImageDepthFile = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED', 'IDEPTH', 'TEST.png' )\n",
    "testCSVFile = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED', 'PCLOUD', 'TEST.csv' )\n",
    "\n",
    "tensorLogDataDir = os.path.join( os.getcwd(), os.pardir, 'TENSOR_LOGS' )\n",
    "\n",
    "imageDirs = ['ICOLOR', 'IDEPTH']\n",
    "csvDirs = ['PCLOUD']\n",
    "\n",
    "allDataFlavors = imageDirs + csvDirs\n",
    "\n",
    "#Load main data file.\n",
    "searCSVInfoFile = os.path.join( combinedDataDir, 'SEAR_DC_INFO.csv' )\n",
    "\n",
    "csDM.CS660DataManagementCheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Data\n",
    "\n",
    "Unless we need to train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL Loaded.\n"
     ]
    }
   ],
   "source": [
    "def loadModel( modelName ):\n",
    "    \"\"\"\n",
    "    Loads the Model.\n",
    "    \"\"\"\n",
    "    theModel = load_model( os.path.join( modelsDirectory, modelName) )\n",
    "    print('MODEL Loaded.')\n",
    "    return theModel\n",
    "\n",
    "\n",
    "theModel = loadModel( 'JUPYTER_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Everything Saved\n"
     ]
    }
   ],
   "source": [
    "def saveModelEverything( theModel, modelName ):\n",
    "    \"\"\"\n",
    "    Saved Everything in regards to the model.\n",
    "    \"\"\"\n",
    "    saveModelStructure( theModel, modelName )\n",
    "    saveModel( theModel, modelName )\n",
    "    saveModelJSON( theModel, modelName )\n",
    "    saveModelWeights( theModel, modelName )\n",
    "    \n",
    "    print(\"Model Everything Saved\")\n",
    "    \n",
    "\n",
    "def saveModelStructure( theModel, modelStructureName ):\n",
    "    \"\"\"\n",
    "    Saves an image of the Model Structure.\n",
    "    \"\"\"\n",
    "    modelStructsFilePath = os.path.join(modelsStructsDirectory, modelStructureName )\n",
    "    plot_model(theModel, to_file=modelStructsFilePath)\n",
    "\n",
    "\n",
    "def saveModelJSON( model, modelName ):\n",
    "    \"\"\"\n",
    "    Saves the Model as JSON\n",
    "    Args:\n",
    "        model: the Keras NN Model\n",
    "        modelName: the Name\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    modelFilePath = os.path.join( modelsDirectory, modelName + '.json' )\n",
    "    model_json = theModel.to_json()\n",
    "    with open( modelFilePath, 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "        \n",
    "def saveModel( model, modelName ):\n",
    "    \"\"\"\n",
    "    Save the model, in Keras [h5] format.\n",
    "    \"\"\"\n",
    "    theModel.save(os.path.join( modelsDirectory, modelName ))\n",
    "\n",
    "    \n",
    "def saveModelWeights( theModel, modelName ):\n",
    "    \"\"\"\n",
    "    Saved the Model Weights\n",
    "    Args:\n",
    "        weights: The Weights\n",
    "        weightName: Weight Names\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    weightsFilePath = os.path.join( weightsDirectory, modelName + '.h5' )\n",
    "    theModel.save_weights( weightsFilePath )\n",
    "    \n",
    "    \n",
    "saveModelEverything( theModel, 'JUPYTER_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the stats on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n",
      "Number of Everything\n",
      "UPs: 278\n",
      "DOWNs: 201\n",
      "NAs: 204\n",
      "HOLEs: 218\n"
     ]
    }
   ],
   "source": [
    "print( len( csDM.getListOfDataCSVFileKeys() ) )\n",
    "csDM.reportStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build a model with Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model with  48  nodes and  1  layers.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 159, 212, 5)       130       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 79, 106, 5)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 39, 52, 10)        460       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 19, 26, 10)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4940)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 48)                237168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 196       \n",
      "=================================================================\n",
      "Total params: 238,146\n",
      "Trainable params: 238,050\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "# from quiver_engine import server\n",
    "\n",
    "# input image dimensions\n",
    "# img_rows, img_cols = 480, 640\n",
    "\n",
    "# num_classes = 4\n",
    "\n",
    "def buildModel( numOfNodes=48, numOfLayers=1):\n",
    "    \"\"\"\n",
    "    Builds the basic model.\n",
    "    Returns:\n",
    "        A Keras NN Model\n",
    "\n",
    "    \"\"\"\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 480, 640\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    num_classes = 4\n",
    "\n",
    "    print(\"Building Model with \", numOfNodes, \" nodes and \", numOfLayers, \" layers.\")\n",
    "\n",
    "    theModel = Sequential()\n",
    "\n",
    "    theModel.add(\n",
    "        Conv2D(5,\n",
    "               kernel_size=(5, 5),\n",
    "               strides=3,\n",
    "               activation='relu',\n",
    "               input_shape=input_shape\n",
    "               )\n",
    "    )\n",
    "    theModel.add(\n",
    "        MaxPooling2D(\n",
    "            pool_size=(2, 2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    theModel.add(\n",
    "        Conv2D(\n",
    "            10,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=2,\n",
    "            activation='relu')\n",
    "    )\n",
    "    theModel.add(\n",
    "        MaxPooling2D(\n",
    "            pool_size=(2, 2),\n",
    "            strides=2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    theModel.add(Flatten())\n",
    "\n",
    "    for index in range( numOfLayers ):\n",
    "        theModel.add(Dense(numOfNodes))\n",
    "        theModel.add(BatchNormalization())\n",
    "        theModel.add(Activation('relu'))\n",
    "        theModel.add(Dropout(0.25))\n",
    "\n",
    "    theModel.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    theModel.compile(\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "\n",
    "    theModel.summary()\n",
    "    \n",
    "    return theModel\n",
    "    \n",
    "# server.launch(model)\n",
    "\n",
    "theModel = buildModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = csDM.load_dataset( '0', 'IDEPTH' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 40 samples\n",
      "Epoch 1/24\n",
      "200/200 [==============================] - 2s - loss: 1.0536 - categorical_accuracy: 0.5950 - val_loss: 3.6376 - val_categorical_accuracy: 0.2500\n",
      "Epoch 2/24\n",
      "200/200 [==============================] - 2s - loss: 0.7032 - categorical_accuracy: 0.7300 - val_loss: 3.4652 - val_categorical_accuracy: 0.2750\n",
      "Epoch 3/24\n",
      "200/200 [==============================] - 2s - loss: 0.6063 - categorical_accuracy: 0.7850 - val_loss: 2.7327 - val_categorical_accuracy: 0.3250\n",
      "Epoch 4/24\n",
      "200/200 [==============================] - 2s - loss: 0.5053 - categorical_accuracy: 0.8450 - val_loss: 1.7159 - val_categorical_accuracy: 0.4500\n",
      "Epoch 5/24\n",
      "200/200 [==============================] - 2s - loss: 0.4295 - categorical_accuracy: 0.8650 - val_loss: 0.9057 - val_categorical_accuracy: 0.7000\n",
      "Epoch 6/24\n",
      "200/200 [==============================] - 2s - loss: 0.3400 - categorical_accuracy: 0.9150 - val_loss: 1.1040 - val_categorical_accuracy: 0.5750\n",
      "Epoch 7/24\n",
      "200/200 [==============================] - 2s - loss: 0.2933 - categorical_accuracy: 0.9250 - val_loss: 1.0530 - val_categorical_accuracy: 0.5750\n",
      "Epoch 8/24\n",
      "200/200 [==============================] - 2s - loss: 0.2851 - categorical_accuracy: 0.9050 - val_loss: 0.9582 - val_categorical_accuracy: 0.6500\n",
      "Epoch 9/24\n",
      "200/200 [==============================] - 2s - loss: 0.2846 - categorical_accuracy: 0.9350 - val_loss: 0.9696 - val_categorical_accuracy: 0.6500\n",
      "Epoch 10/24\n",
      "200/200 [==============================] - 2s - loss: 0.1951 - categorical_accuracy: 0.9550 - val_loss: 0.9553 - val_categorical_accuracy: 0.7000\n",
      "Epoch 11/24\n",
      "200/200 [==============================] - 2s - loss: 0.1508 - categorical_accuracy: 0.9800 - val_loss: 0.9049 - val_categorical_accuracy: 0.6250\n",
      "Epoch 12/24\n",
      "200/200 [==============================] - 2s - loss: 0.2163 - categorical_accuracy: 0.9600 - val_loss: 1.0445 - val_categorical_accuracy: 0.5750\n",
      "Epoch 13/24\n",
      "200/200 [==============================] - 2s - loss: 0.2038 - categorical_accuracy: 0.9500 - val_loss: 0.8082 - val_categorical_accuracy: 0.7500\n",
      "Epoch 14/24\n",
      "200/200 [==============================] - 2s - loss: 0.1296 - categorical_accuracy: 0.9600 - val_loss: 0.8386 - val_categorical_accuracy: 0.7000\n",
      "Epoch 15/24\n",
      "200/200 [==============================] - 2s - loss: 0.1348 - categorical_accuracy: 0.9700 - val_loss: 0.8065 - val_categorical_accuracy: 0.7000\n",
      "Epoch 16/24\n",
      "200/200 [==============================] - 2s - loss: 0.1211 - categorical_accuracy: 0.9700 - val_loss: 0.7848 - val_categorical_accuracy: 0.7000\n",
      "Epoch 17/24\n",
      "200/200 [==============================] - 2s - loss: 0.1203 - categorical_accuracy: 0.9750 - val_loss: 1.0314 - val_categorical_accuracy: 0.6250\n",
      "Epoch 18/24\n",
      "200/200 [==============================] - 2s - loss: 0.1370 - categorical_accuracy: 0.9550 - val_loss: 0.9047 - val_categorical_accuracy: 0.7000\n",
      "Epoch 19/24\n",
      "200/200 [==============================] - 2s - loss: 0.1297 - categorical_accuracy: 0.9600 - val_loss: 0.8782 - val_categorical_accuracy: 0.6500\n",
      "Epoch 20/24\n",
      "200/200 [==============================] - 2s - loss: 0.1147 - categorical_accuracy: 0.9800 - val_loss: 0.8253 - val_categorical_accuracy: 0.7000\n",
      "Epoch 21/24\n",
      "200/200 [==============================] - 2s - loss: 0.1167 - categorical_accuracy: 0.9750 - val_loss: 0.8008 - val_categorical_accuracy: 0.7000\n",
      "Epoch 22/24\n",
      "200/200 [==============================] - 2s - loss: 0.0981 - categorical_accuracy: 1.0000 - val_loss: 0.7823 - val_categorical_accuracy: 0.6750\n",
      "Epoch 23/24\n",
      "200/200 [==============================] - 2s - loss: 0.0966 - categorical_accuracy: 0.9900 - val_loss: 0.8237 - val_categorical_accuracy: 0.7250\n",
      "Epoch 24/24\n",
      "200/200 [==============================] - 2s - loss: 0.0779 - categorical_accuracy: 0.9850 - val_loss: 0.9520 - val_categorical_accuracy: 0.6250\n"
     ]
    }
   ],
   "source": [
    "def trainModel( trainingName, theModel, x_train, y_train, x_test, y_test, num_classes=4 ):\n",
    "    \"\"\"\n",
    "    Trains the model via given data.\n",
    "\n",
    "    Args:\n",
    "        trainingName: A name of this train [mainly to track in TensorBoard\n",
    "        x_train: The X Set for Trainings\n",
    "        y_train: The Y set for Testing\n",
    "        x_test:  The X Set for Training/Verification\n",
    "        y_test:  The Y Set for Testing/Verification\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    img_rows, img_cols = 480, 640\n",
    "    \n",
    "    # Reshape the X sets.\n",
    "    # Mainly for this project.because Keras/Tensor thinks in Channels.\n",
    "    # And since we are using Greyscale data, we really don't have a channel.\n",
    "    # So we have to 'fake' a channel\n",
    "    #\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "    # Convert class vectors to binary class matrices\n",
    "    y_train_as_category = to_categorical(y_train, num_classes)\n",
    "    y_test_as_category = to_categorical(y_test, num_classes)\n",
    "\n",
    "    logFilePath = os.path.join( tensorLogDataDir, trainingName )\n",
    "\n",
    "    TBoardCallback = keras.callbacks.TensorBoard(\n",
    "        log_dir=logFilePath,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_images=True\n",
    "    )\n",
    "\n",
    "    theModel.fit(x_train,\n",
    "              y_train_as_category,\n",
    "              batch_size=16,\n",
    "              epochs=24,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test_as_category),\n",
    "              callbacks=[TBoardCallback]\n",
    "              )\n",
    "    \n",
    "    return theModel\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "# X_test = X_test.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# # convert class vectors to binary class matrices\n",
    "# y_train_as_category = to_categorical(y_train, num_classes)\n",
    "# y_test_as_category = to_categorical(y_test, num_classes)\n",
    "\n",
    "theModel = trainModel( \"JupyterTESTRUN\", theModel, X_train, Y_train, X_test, Y_test, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset for Testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_Z, Y_train_Z, X_test_Z, Y_test_Z = csDM.load_dataset( '0', 'IDEPTH' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General > Test loss:  0.809247305989 Test accuracy:  0.675\n",
      "40/40 [==============================] - 0s     \n",
      "[1.0, 0.6, 0.4, 0.7]\n"
     ]
    }
   ],
   "source": [
    "def evaluateModel( theModel, x_test, y_test, num_classes):\n",
    "    \"\"\"\n",
    "    Evaluated the Model.\n",
    "    \n",
    "    Parameters:\n",
    "        theModel:\n",
    "        x_test:\n",
    "        y_test:\n",
    "        num_classes:\n",
    "        \n",
    "    Return:\n",
    "    \n",
    "    \"\"\"\n",
    "    img_rows, img_cols = 480, 640\n",
    "    \n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    y_test_as_category = to_categorical(y_test, num_classes)\n",
    "\n",
    "    score = theModel.evaluate(x_test, y_test_as_category, verbose=0)\n",
    "    print('General > Test loss: ', score[0], 'Test accuracy: ', score[1] )\n",
    "\n",
    "    predictionResults = theModel.predict_classes(x_test, verbose=1)\n",
    "    \n",
    "    scoringList = [0, 0, 0, 0]\n",
    "    scoringListAsPecents = []\n",
    "    \n",
    "    for index in range(len(x_test)):\n",
    "        if( predictionResults[index] == y_test[index] ):\n",
    "#             print( index, 'Results: ', predictionResults[index], \" VS \", y_test[index], \"Match\" )\n",
    "            scoringList[ int(y_test[index]) ] = scoringList[ int(y_test[index]) ] + 1\n",
    "#         else:\n",
    "#             print( index, 'Results: ', predictionResults[index], \" VS \", y_test[index], \"No Match\" )\n",
    "    \n",
    "    for element in scoringList:\n",
    "        scoringListAsPecents.append( element / 10.0 )\n",
    "    \n",
    "#     print( scoringList )\n",
    "        \n",
    "    return { 'SCORE': score, 'SCORELIST' : scoringListAsPecents }\n",
    "\n",
    "\n",
    "# scoringResults = evaluateModel( theModel, X_test, Y_test, 4)\n",
    "scoringResults = evaluateModel( theModel, X_test_Z, Y_test_Z, 4)\n",
    "print(scoringResults['SCORELIST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Human Oracle Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 200 200\n",
      "\n",
      "220\n",
      "2\n",
      "DOWN\n",
      "\n",
      "[15082667660, 15085989050, 15087883470, 15084376710, 15085985661, 15084377460, 15085985011, 15087879680, 15085984771, 15084365231, 15082659990, 15085991310, 15085994271, 15087884390, 15085984830, 15082666740, 15084364850, 15084364850, 15085994200, 15082667661]\n"
     ]
    }
   ],
   "source": [
    "def reportOracle( status ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oracleReportPath = os.path.join(resultsDirectory, 'JUPYTER_ORACLE_REPORT.txt')\n",
    "    fileToWrite = open(oracleReportPath, 'a')\n",
    "    fileToWrite.write( status + '\\n' )\n",
    "    fileToWrite.close()\n",
    "\n",
    "\n",
    "def getLowestScoringCategory( scoringListAsPecents ):\n",
    "    \"\"\"\n",
    "    Get the index of the lowest scoring category.\n",
    "    \"\"\"\n",
    "    \n",
    "    lowestCategoryPercent = 1.0\n",
    "    currentLowest = 0\n",
    "    \n",
    "    for index in range(len(scoringListAsPecents)):\n",
    "        currentCategoryPercent = scoringListAsPecents[index]\n",
    "        if( currentCategoryPercent < lowestCategoryPercent ):\n",
    "            currentLowest = index\n",
    "            lowestCategoryPercent = currentCategoryPercent\n",
    "\n",
    "    return currentLowest\n",
    "\n",
    "\n",
    "def addNewSamplesToTrainingSet( trainingList, newSamples ):\n",
    "    \"\"\"\n",
    "    Merge the new samples with the Training set.\n",
    "    \"\"\"\n",
    "    return trainingList + newSamples\n",
    "    \n",
    "\n",
    "def getSamplesFromAClass( classType, trainingList, allTestListsCombined, numberOfSamples ):\n",
    "    \"\"\"\n",
    "    Get multiple samples from a class.\n",
    "    \"\"\"\n",
    "    newSamples = []\n",
    "    \n",
    "    for index in range(numberOfSamples):\n",
    "        newSample = getASampleOfClass( classType, trainingList, allTestListsCombined )\n",
    "        newSamples.append( newSample )\n",
    "\n",
    "    return newSamples\n",
    "        \n",
    "\n",
    "def getASampleOfClass( classType, trainingList, allTestListsCombined ):\n",
    "    \"\"\"\n",
    "    Get a Sample from a Specific class that isn't being trained on yet.\n",
    "    \"\"\"\n",
    "    \n",
    "    if( classType == 'UP' ):\n",
    "        classType = 'upList'\n",
    "    elif( classType == 'DOWN' ):\n",
    "        classType = 'downList'\n",
    "    elif( classType == 'NA' ):\n",
    "        classType = 'naList'\n",
    "    elif( classType == 'HOLE' ):\n",
    "        classType == 'holeList'\n",
    "        \n",
    "    random.seed( datetime.datetime.utcnow() )\n",
    "    \n",
    "    dataClassList = csDM.getDictOfClassLists()\n",
    "    \n",
    "    trainingListAsSet = set( trainingList )\n",
    "    allTestListsCombinedAsSet = set( allTestListsCombined )\n",
    "    \n",
    "    allElementsOfClass = dataClassList[classType]\n",
    "    \n",
    "    selectedSample = random.choice(allElementsOfClass)\n",
    "    \n",
    "    while( ( selectedSample in trainingListAsSet) or ( selectedSample in allTestListsCombinedAsSet) ):\n",
    "        selectedSample = random.choice(allElementsOfClass)\n",
    "\n",
    "    return selectedSample\n",
    "    \n",
    "\n",
    "def getAllTestLists():\n",
    "    \"\"\"\n",
    "    Gets all the test lists.\n",
    "    Returns:\n",
    "        A Dict of the test lists.\n",
    "    \"\"\"\n",
    "    allTestLists = []\n",
    "    allTestListCombined = []\n",
    "    \n",
    "    for index in range(5):\n",
    "        allTestLists.append( csDM.getTestList(index) )\n",
    "        allTestListCombined = allTestListCombined + csDM.getTestList(index)\n",
    "        \n",
    "    return { 'TestLists' : allTestLists, 'CombinedTestLists' : allTestListCombined}\n",
    "\n",
    "dataClassList = csDM.getDictOfClassLists()\n",
    "allTheTestLists = getAllTestLists()\n",
    "theTrainingList = csDM.getTrainingList(0)\n",
    "\n",
    "print( len( allTheTestLists['TestLists'] ), len( allTheTestLists['CombinedTestLists'] ), len(theTrainingList) )\n",
    "print(\"\")\n",
    "\n",
    "combinedTrainingAndNewSampleList = addNewSamplesToTrainingSet( theTrainingList, newTrainingSet )\n",
    "lowestCategory = csDM.getClassFromNumeral(getLowestScoringCategory(scoringResults['SCORELIST']))\n",
    "newTrainingSet = getSamplesFromAClass( lowestCategory, theTrainingList, allTheTestLists['CombinedTestLists'], 20 )\n",
    "\n",
    "print(len(combinedTrainingAndNewSampleList))\n",
    "print(getLowestScoringCategory(scoringResults['SCORELIST']))\n",
    "print(lowestCategory)\n",
    "\n",
    "print(\"\")\n",
    "print(newTrainingSet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
