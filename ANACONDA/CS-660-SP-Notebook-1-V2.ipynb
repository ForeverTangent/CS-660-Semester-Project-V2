{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-660 Semester Project Notebook 1\n",
    "\n",
    "## Stan Rosenbaum\n",
    "\n",
    "Fall 2017\n",
    "\n",
    "Anaconda 5 / Python 3\n",
    "\n",
    "Using Keras with TensorFlow as back end.\n",
    "\n",
    "### Background Important Stuff\n",
    "\n",
    "First the Enums of the Classes\n",
    "\n",
    "* \"NA\" = 0\n",
    "* \"UP\" = 1\n",
    "* \"DOWN\" = 2\n",
    "* \"HOLE\" = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS660DataManagementCheck Imported\n"
     ]
    }
   ],
   "source": [
    "# First we init stuff.\n",
    "# Load the Basic Python Libraries\n",
    "import os\n",
    "import csv\n",
    "import PIL\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "# Load my Data Management Module\n",
    "import CS660DataManagement as csDM\n",
    "\n",
    "# load numpy\n",
    "import numpy as np\n",
    "\n",
    "# Load Keras Stuff\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "# Other.  Mostly Graphic stuff for displaying Data in and out of Jupyter.\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import pydot\n",
    "import graphviz\n",
    "from IPython.display import SVG\n",
    "\n",
    "# Not using Quiver Yet.\n",
    "# from quiver_engine import server\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Get Processed Data Directory.\n",
    "processedDataDir = os.path.join( os.getcwd(), os.pardir, 'DATA', 'PROCESSED' )\n",
    "combinedDataDir = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED' )\n",
    "pickleDataDir = os.path.join( os.getcwd(), os.pardir, 'DATA', 'PICKLES' )\n",
    "modelsDirectory = os.path.join( os.getcwd(), os.pardir, 'MODELS' )\n",
    "modelsStructsDirectory = os.path.join( os.getcwd(), os.pardir, 'MODELS_STRUCTS' )\n",
    "weightsDirectory = os.path.join( os.getcwd(), os.pardir, 'WEIGHTS' )\n",
    "resultsDirectory = os.path.join( os.getcwd(), os.pardir, 'RESULTS' )\n",
    "\n",
    "testImageColorFile = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED', 'ICOLOR', 'TEST.png' )\n",
    "testImageDepthFile = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED', 'IDEPTH', 'TEST.png' )\n",
    "testCSVFile = os.path.join( os.getcwd(), os.pardir, 'DATA', 'COMBINED', 'PCLOUD', 'TEST.csv' )\n",
    "\n",
    "tensorLogDataDir = os.path.join( os.getcwd(), os.pardir, 'TENSOR_LOGS' )\n",
    "\n",
    "imageDirs = ['ICOLOR', 'IDEPTH']\n",
    "csvDirs = ['PCLOUD']\n",
    "\n",
    "allDataFlavors = imageDirs + csvDirs\n",
    "\n",
    "#Load main data file.\n",
    "searCSVInfoFile = os.path.join( combinedDataDir, 'SEAR_DC_INFO.csv' )\n",
    "\n",
    "csDM.CS660DataManagementCheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Data\n",
    "\n",
    "Unless we need to train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL Loaded.\n"
     ]
    }
   ],
   "source": [
    "def loadModel( modelName ):\n",
    "    \"\"\"\n",
    "    Loads the Model.\n",
    "    \"\"\"\n",
    "    theModel = load_model( os.path.join( modelsDirectory, modelName) )\n",
    "    print('MODEL Loaded.')\n",
    "    return theModel\n",
    "\n",
    "\n",
    "theModel = loadModel( 'JUPYTER_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Everything Saved\n"
     ]
    }
   ],
   "source": [
    "def saveModelEverything( theModel, modelName ):\n",
    "    \"\"\"\n",
    "    Saved Everything in regards to the model.\n",
    "    \"\"\"\n",
    "    saveModelStructure( theModel, modelName )\n",
    "    saveModel( theModel, modelName )\n",
    "    saveModelJSON( theModel, modelName )\n",
    "    saveModelWeights( theModel, modelName )\n",
    "    \n",
    "    print(\"Model Everything Saved\")\n",
    "    \n",
    "\n",
    "def saveModelStructure( theModel, modelStructureName ):\n",
    "    \"\"\"\n",
    "    Saves an image of the Model Structure.\n",
    "    \"\"\"\n",
    "    modelStructsFilePath = os.path.join(modelsStructsDirectory, modelStructureName )\n",
    "    plot_model(theModel, to_file=modelStructsFilePath)\n",
    "\n",
    "\n",
    "def saveModelJSON( model, modelName ):\n",
    "    \"\"\"\n",
    "    Saves the Model as JSON\n",
    "    Args:\n",
    "        model: the Keras NN Model\n",
    "        modelName: the Name\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    modelFilePath = os.path.join( modelsDirectory, modelName + '.json' )\n",
    "    model_json = theModel.to_json()\n",
    "    with open( modelFilePath, 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "        \n",
    "def saveModel( model, modelName ):\n",
    "    \"\"\"\n",
    "    Save the model, in Keras [h5] format.\n",
    "    \"\"\"\n",
    "    theModel.save(os.path.join( modelsDirectory, modelName ))\n",
    "\n",
    "    \n",
    "def saveModelWeights( theModel, modelName ):\n",
    "    \"\"\"\n",
    "    Saved the Model Weights\n",
    "    Args:\n",
    "        weights: The Weights\n",
    "        weightName: Weight Names\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    weightsFilePath = os.path.join( weightsDirectory, modelName + '.h5' )\n",
    "    theModel.save_weights( weightsFilePath )\n",
    "    \n",
    "    \n",
    "saveModelEverything( theModel, 'JUPYTER_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the stats on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n",
      "Number of Everything\n",
      "UPs: 278\n",
      "DOWNs: 201\n",
      "NAs: 204\n",
      "HOLEs: 218\n"
     ]
    }
   ],
   "source": [
    "print( len( csDM.getListOfDataCSVFileKeys() ) )\n",
    "csDM.reportStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build a model with Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from quiver_engine import server\n",
    "\n",
    "# input image dimensions\n",
    "# img_rows, img_cols = 480, 640\n",
    "\n",
    "# num_classes = 4\n",
    "\n",
    "def buildModel( numOfNodes=48, numOfLayers=1):\n",
    "    \"\"\"\n",
    "    Builds the basic model.\n",
    "    Returns:\n",
    "        A Keras NN Model\n",
    "\n",
    "    \"\"\"\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 480, 640\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    num_classes = 4\n",
    "\n",
    "    print(\"Building Model with \", numOfNodes, \" nodes and \", numOfLayers, \" layers.\")\n",
    "\n",
    "    theModel = Sequential()\n",
    "\n",
    "    theModel.add(\n",
    "        Conv2D(5,\n",
    "               kernel_size=(5, 5),\n",
    "               strides=3,\n",
    "               activation='relu',\n",
    "               input_shape=input_shape\n",
    "               )\n",
    "    )\n",
    "    theModel.add(\n",
    "        MaxPooling2D(\n",
    "            pool_size=(2, 2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    theModel.add(\n",
    "        Conv2D(\n",
    "            10,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=2,\n",
    "            activation='relu')\n",
    "    )\n",
    "    theModel.add(\n",
    "        MaxPooling2D(\n",
    "            pool_size=(2, 2),\n",
    "            strides=2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    theModel.add(Flatten())\n",
    "\n",
    "    for index in range( numOfLayers ):\n",
    "        theModel.add(Dense(numOfNodes))\n",
    "        theModel.add(BatchNormalization())\n",
    "        theModel.add(Activation('relu'))\n",
    "        theModel.add(Dropout(0.25))\n",
    "\n",
    "    theModel.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    theModel.compile(\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "\n",
    "    theModel.summary()\n",
    "    \n",
    "    return theModel\n",
    "    \n",
    "# server.launch(model)\n",
    "\n",
    "# theModel = buildModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = csDM.loadTrainingAndTestDataset( '0', 'IDEPTH' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel( trainingName, theModel, x_train, y_train, x_test, y_test, num_classes=4 ):\n",
    "    \"\"\"\n",
    "    Trains the model via given data.\n",
    "\n",
    "    Args:\n",
    "        trainingName: A name of this train [mainly to track in TensorBoard\n",
    "        x_train: The X Set for Trainings\n",
    "        y_train: The Y set for Testing\n",
    "        x_test:  The X Set for Training/Verification\n",
    "        y_test:  The Y Set for Testing/Verification\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    img_rows, img_cols = 480, 640\n",
    "    \n",
    "    # Reshape the X sets.\n",
    "    # Mainly for this project.because Keras/Tensor thinks in Channels.\n",
    "    # And since we are using Greyscale data, we really don't have a channel.\n",
    "    # So we have to 'fake' a channel\n",
    "    #\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "    # Convert class vectors to binary class matrices\n",
    "    y_train_as_category = to_categorical(y_train, num_classes)\n",
    "    y_test_as_category = to_categorical(y_test, num_classes)\n",
    "\n",
    "    logFilePath = os.path.join( tensorLogDataDir, trainingName )\n",
    "\n",
    "    TBoardCallback = keras.callbacks.TensorBoard(\n",
    "        log_dir=logFilePath,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_images=True\n",
    "    )\n",
    "\n",
    "    theModel.fit(x_train,\n",
    "              y_train_as_category,\n",
    "              batch_size=16,\n",
    "              epochs=24,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test_as_category),\n",
    "              callbacks=[TBoardCallback]\n",
    "              )\n",
    "    \n",
    "    return theModel\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "# X_test = X_test.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# # convert class vectors to binary class matrices\n",
    "# y_train_as_category = to_categorical(y_train, num_classes)\n",
    "# y_test_as_category = to_categorical(y_test, num_classes)\n",
    "\n",
    "# theModel = trainModel( \"JupyterTESTRUN\", theModel, X_train, Y_train, X_test, Y_test, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset for Testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_Z, Y_test_Z = csDM.loadTestOnlyDataset( '2', 'IDEPTH' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel( theModel, x_test, y_test, num_classes):\n",
    "    \"\"\"\n",
    "    Evaluated the Model.\n",
    "    \n",
    "    Parameters:\n",
    "        theModel:\n",
    "        x_test:\n",
    "        y_test:\n",
    "        num_classes:\n",
    "        \n",
    "    Return:\n",
    "    \n",
    "    \"\"\"\n",
    "    img_rows, img_cols = 480, 640\n",
    "    \n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    y_test_as_category = to_categorical(y_test, num_classes)\n",
    "\n",
    "    score = theModel.evaluate(x_test, y_test_as_category, verbose=0)\n",
    "    print('General > Test loss: ', score[0], 'Test accuracy: ', score[1] )\n",
    "\n",
    "    predictionResults = theModel.predict_classes(x_test, verbose=1)\n",
    "    \n",
    "    scoringList = [0, 0, 0, 0]\n",
    "    scoringListAsPecents = []\n",
    "    \n",
    "    for index in range(len(x_test)):\n",
    "        if( predictionResults[index] == y_test[index] ):\n",
    "#             print( index, 'Results: ', predictionResults[index], \" VS \", y_test[index], \"Match\" )\n",
    "            scoringList[ int(y_test[index]) ] = scoringList[ int(y_test[index]) ] + 1\n",
    "#         else:\n",
    "#             print( index, 'Results: ', predictionResults[index], \" VS \", y_test[index], \"No Match\" )\n",
    "    \n",
    "    for element in scoringList:\n",
    "        scoringListAsPecents.append( element / 10.0 )\n",
    "    \n",
    "#     print( scoringList )\n",
    "        \n",
    "    return { 'SCORE': score, 'SCORELIST' : scoringListAsPecents }\n",
    "\n",
    "\n",
    "# scoringResults = evaluateModel( theModel, X_test, Y_test, 4)\n",
    "# scoringResults = evaluateModel( theModel, X_test_Z, Y_test_Z, 4)\n",
    "# print(scoringResults['SCORELIST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Human Oracle Part\n",
    "\n",
    "1. Running Training\n",
    "2. Analysize Training with Test Set via Predict()\n",
    "    1. Record Results\n",
    "3. Get the class which got the lowest score from Prediction.\n",
    "    1. Record that Class\n",
    "4. Get 20? Elements of that Class as the Oracle.\n",
    "    1. Record what was added\n",
    "5. Add those new elements into the Training \n",
    "6. Go to Step 1, X Times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 200 200\n",
      "\n",
      "220\n",
      "2\n",
      "DOWN\n",
      "\n",
      "[15087887930, 15085985731, 15106037831, 15082667490, 15084368131, 15085985381, 15084365231, 15087878110, 15085991441, 15082669790, 15082668191, 15085991180, 15082659871, 15082676080, 15082668120, 15087880940, 15084375431, 15082675710, 15087879680, 15084375540]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def runHumanOracle():\n",
    "    \"\"\"\n",
    "    Human Oracle master function\n",
    "    \"\"\"\n",
    "    # What are re analysizing.\n",
    "    dataFlavor = 'IDEPTH'\n",
    "    \n",
    "    # First get the file name we need to record data.\n",
    "    trainingPredictionResultsFileName = getTrainingPredictionResultsFileName()\n",
    "    elementsAddedToTrainingSetFileName = getElementsAddedToTrainingSetFileName()\n",
    "    \n",
    "    # Get Elements to train and test on\n",
    "    X_train, Y_train, X_test, Y_test = csDM.loadTrainingAndTestDataset( '0', dataFlavor )\n",
    "    \n",
    "    # This is to ensure our test set always stay independent.\n",
    "    allTestListsCombined = getAllTestLists()\n",
    "    \n",
    "    # Get Independent Set for Testing \n",
    "    # Techincally the above X_test, Y_test, should be indepedent, according to the Keras documentation.\n",
    "    # But I am using a second set just because.\n",
    "    X_test_Z, Y_test_Z = csDM.loadTestOnlyDataset( '2', dataFlavor )\n",
    "    \n",
    "    # Build Model\n",
    "    theModel = buildModel()\n",
    "    \n",
    "    # Train the Model\n",
    "    trainModel( \"JupyterHumanOracleTraining\", theModel, X_train, Y_train, X_test, Y_test, 4)\n",
    "        \n",
    "    # Evaluate the Model [with indie data]\n",
    "    scoringResults = evaluateModel( theModel, X_test_Z, Y_test_Z, 4)\n",
    "\n",
    "    # Now the real Fun starts.\n",
    "    # Get Lowest scoring class.\n",
    "    lowestScoringClassName = csDM.getClassFromNumeral( getLowestScoringCategory( scoringResults['SCORELIST'] ) )\n",
    "\n",
    "    # Record the Init Results of the first eval and the first lowest scoring category.\n",
    "    recordTrainingPredictionResults( trainingPredictionResultsFileName, scoringResults, lowestScoringClassName )\n",
    "\n",
    "    for index in range(3):\n",
    "\n",
    "        # Get new samples from the lowest scoring class.\n",
    "        newSamples = getSamplesFromAClass( lowestScoringClassName, X_train_to_expand, allTestListsCombined, 20 )    \n",
    "\n",
    "        # Record What we added.\n",
    "        recordElementsAddedToTrainingSet( elementsAddedToTrainingSetFileName, lowestScoringClassName, newSamples )\n",
    "\n",
    "        # Add samples to training set\n",
    "        # NOTE: We need to turn the 'newSamples' into NPArrays.  It is these new NP Arrays we add to \n",
    "        # X_train, Y_train.  Out Original List of the what is in the training sets stay intact.\n",
    "\n",
    "        # So first get the NPArrays of the new Samples\n",
    "        dictOfLearningAndVerificationNPArrays = createNPArraysFor( newSamples, dataFlavor )\n",
    "\n",
    "        # Then we add them to the training set.\n",
    "        X_train = np.concatenate( X_train, dictOfLearningAndVerificationNPArrays['LEARNING'] )\n",
    "        Y_train = np.concatenate( Y_train, dictOfLearningAndVerificationNPArrays['VERIFICATION'] )\n",
    "\n",
    "        # And Then we train the model again.\n",
    "        stringOfTheAge = 'JupyterHumanOracleTraining_AGE_' + str(index)\n",
    "        trainModel( stringOfTheAge, theModel, X_train, Y_train, X_test, Y_test, 4)\n",
    "        \n",
    "        # Record the Init Results of the first eval and the first lowest scoring category.\n",
    "        recordTrainingPredictionResults( trainingPredictionResultsFileName, scoringResults, lowestScoringClassName )\n",
    "\n",
    "    \n",
    "    \n",
    "def getElementsAddedToTrainingSetFileName():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    theFileName = 'JUPYTER_ADDED_ELEMENTS_RECORD' + csDM.getDateTimeAsString() + '.txt'\n",
    "    return theFileName\n",
    "\n",
    "\n",
    "def getTrainingPredictionResultsFileName():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    theFileName = 'JUPYTER_TRAIN_PREDICTION_RESULTS' + csDM.getDateTimeAsString() + '.txt'\n",
    "    return theFileName\n",
    "\n",
    "\n",
    "    \n",
    "def recordElementsAddedToTrainingSet( currentFileName, nameOfClassAdded, elementsList ):\n",
    "    \"\"\"\n",
    "    Record the Training and Predictions Results.\n",
    "    \"\"\"\n",
    "    trainingPredictionResultsPath = os.path.join(resultsDirectory, currentFileName )\n",
    "\n",
    "    classAndElements = []\n",
    "    classAndElements.append(nameOfClassAdded)\n",
    "    classAndElements = classAndElements + elementsList\n",
    "    \n",
    "    print(classAndElements)\n",
    "    \n",
    "    with open(trainingPredictionResultsPath, 'a', newline='\\n') as csvfile:\n",
    "        csvWriter = csv.writer(\n",
    "            csvfile, \n",
    "            delimiter=','\n",
    "        )\n",
    "        csvWriter.writerow( classAndElements )\n",
    "\n",
    "            \n",
    "def recordTrainingPredictionResults( currentFileName, scoringResults, lowestScoringClassName ):\n",
    "    \"\"\"\n",
    "    Record the Training and Predictions Results.\n",
    "    \"\"\"\n",
    "    trainingPredictionResultsPath = os.path.join(resultsDirectory, currentFileName)\n",
    "    allScoringInfo = scoringResults['SCORE'] + scoringResults['SCORELIST']\n",
    "    allScoringInfo.append(lowestScoringClassName)\n",
    "    \n",
    "    print(allScoringInfo)\n",
    "    \n",
    "    if( not os.path.exists( trainingPredictionResultsPath) ):\n",
    "        headers = ['TestLoss', 'TestAccuracy', 'NA', 'UP', 'DOWN', 'HOLE']\n",
    "        with open(trainingPredictionResultsPath, 'w', newline='\\n') as csvfile:\n",
    "            csvWriter = csv.writer(\n",
    "                csvfile, \n",
    "                delimiter=','\n",
    "            )\n",
    "            csvWriter.writerow( headers )\n",
    "            csvWriter.writerow( allScoringInfo )\n",
    "            \n",
    "    else:\n",
    "        print(allScoringInfo)\n",
    "        with open(trainingPredictionResultsPath, 'a', newline='\\n') as csvfile:\n",
    "            csvWriter = csv.writer(\n",
    "                csvfile, \n",
    "                delimiter=','\n",
    "            )\n",
    "            csvWriter.writerow( allScoringInfo )\n",
    "\n",
    "\n",
    "def reportOracle( status ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oracleReportPath = os.path.join(resultsDirectory, 'JUPYTER_ORACLE_REPORT.txt')\n",
    "    fileToWrite = open(oracleReportPath, 'a')\n",
    "    fileToWrite.write( status + '\\n' )\n",
    "    fileToWrite.close()\n",
    "\n",
    "\n",
    "def getLowestScoringCategory( scoringListAsPecents ):\n",
    "    \"\"\"\n",
    "    Get the index of the lowest scoring category.\n",
    "    \"\"\"\n",
    "    \n",
    "    lowestCategoryPercent = 1.0\n",
    "    currentLowestIndex = 0\n",
    "    \n",
    "    for index in range(len(scoringListAsPecents)):\n",
    "        currentCategoryPercent = scoringListAsPecents[index]\n",
    "        if( currentCategoryPercent < lowestCategoryPercent ):\n",
    "            currentLowestIndex = index\n",
    "            lowestCategoryPercent = currentCategoryPercent\n",
    "\n",
    "    return currentLowestIndex\n",
    "\n",
    "\n",
    "def addNewSamplesToTrainingSet( trainingList, newSamples ):\n",
    "    \"\"\"\n",
    "    Merge the new samples with the Training set.\n",
    "    \"\"\"\n",
    "    return trainingList + newSamples\n",
    "    \n",
    "\n",
    "def getSamplesFromAClass( classType, trainingList, allTestListsCombined, numberOfSamples ):\n",
    "    \"\"\"\n",
    "    Get multiple samples from a class.\n",
    "    \"\"\"\n",
    "    newSamples = []\n",
    "    \n",
    "    for index in range(numberOfSamples):\n",
    "        newSample = getASampleOfClass( classType, trainingList, allTestListsCombined )\n",
    "        newSamples.append( newSample )\n",
    "\n",
    "    return newSamples\n",
    "        \n",
    "\n",
    "def getASampleOfClass( classType, trainingList, allTestListsCombined ):\n",
    "    \"\"\"\n",
    "    Get a Sample from a Specific class that isn't being trained on yet.\n",
    "    \"\"\"\n",
    "    \n",
    "    if( classType == 'UP' ):\n",
    "        classType = 'upList'\n",
    "    elif( classType == 'DOWN' ):\n",
    "        classType = 'downList'\n",
    "    elif( classType == 'NA' ):\n",
    "        classType = 'naList'\n",
    "    elif( classType == 'HOLE' ):\n",
    "        classType == 'holeList'\n",
    "        \n",
    "    random.seed( datetime.datetime.utcnow() )\n",
    "    \n",
    "    dataClassList = csDM.getDictOfClassLists()\n",
    "    \n",
    "    trainingListAsSet = set( trainingList )\n",
    "    allTestListsCombinedAsSet = set( allTestListsCombined )\n",
    "    \n",
    "    allElementsOfClass = dataClassList[classType]\n",
    "    \n",
    "    selectedSample = random.choice(allElementsOfClass)\n",
    "    \n",
    "    while( ( selectedSample in trainingListAsSet) or ( selectedSample in allTestListsCombinedAsSet) ):\n",
    "        selectedSample = random.choice(allElementsOfClass)\n",
    "\n",
    "    return selectedSample\n",
    "    \n",
    "\n",
    "def getAllTestLists():\n",
    "    \"\"\"\n",
    "    Gets all the test lists.\n",
    "    Returns:\n",
    "        A Dict of the test lists.\n",
    "    \"\"\"\n",
    "    allTestLists = []\n",
    "    allTestListCombined = []\n",
    "    \n",
    "    for index in range(5):\n",
    "        allTestLists.append( csDM.getTestList(index) )\n",
    "        allTestListCombined = allTestListCombined + csDM.getTestList(index)\n",
    "        \n",
    "    return { 'TestLists' : allTestLists, 'CombinedTestLists' : allTestListCombined}\n",
    "\n",
    "dataClassList = csDM.getDictOfClassLists()\n",
    "allTheTestLists = getAllTestLists()\n",
    "theTrainingList = csDM.getTrainingList(0)\n",
    "\n",
    "print( len( allTheTestLists['TestLists'] ), len( allTheTestLists['CombinedTestLists'] ), len(theTrainingList) )\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "lowestCategory = csDM.getClassFromNumeral(getLowestScoringCategory(scoringResults['SCORELIST']))\n",
    "newTrainingSet = getSamplesFromAClass( lowestCategory, theTrainingList, allTheTestLists['CombinedTestLists'], 20 )\n",
    "combinedTrainingAndNewSampleList = addNewSamplesToTrainingSet( theTrainingList, newTrainingSet )\n",
    "\n",
    "print(len(combinedTrainingAndNewSampleList))\n",
    "print(getLowestScoringCategory(scoringResults['SCORELIST']))\n",
    "print(lowestCategory)\n",
    "\n",
    "print(\"\")\n",
    "print(newTrainingSet)\n",
    "\n",
    "trainingPredictionResultsFileName = getTrainingPredictionResultsFileName()\n",
    "elementsAddedToTrainingSetFileName = getElementsAddedToTrainingSetFileName()\n",
    "\n",
    "# runHumanOracle()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
